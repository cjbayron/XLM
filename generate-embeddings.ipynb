{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2019-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Code to generate sentence representations from a pretrained model.\n",
    "# This can be used to initialize a cross-lingual classifier, for instance.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from src.utils import AttrDict\n",
    "from src.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from src.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n"
     ]
    }
   ],
   "source": [
    "# model_path = 'models/mlm_100_1280.pth'\n",
    "# model_path = 'mlm_tlm_xnli15_1024.pth'\n",
    "model_path = 'mlm_enfr_1024.pth'\n",
    "\n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dico_id2word', 'dico_word2id', 'dico_counts', 'params'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'same_enc_dec': True,\n",
       " 'emb_dim': 1024,\n",
       " 'n_layers': 12,\n",
       " 'n_heads': 8,\n",
       " 'dropout': 0.1,\n",
       " 'attention_dropout': 0.1,\n",
       " 'gelu_activation': True,\n",
       " 'share_inout_emb': True,\n",
       " 'sinusoidal_embeddings': False,\n",
       " 'asm': False,\n",
       " 'max_vocab': 95000,\n",
       " 'min_count': 0,\n",
       " 'id2lang': {0: 'ar',\n",
       "  1: 'bg',\n",
       "  2: 'de',\n",
       "  3: 'el',\n",
       "  4: 'en',\n",
       "  5: 'es',\n",
       "  6: 'fr',\n",
       "  7: 'hi',\n",
       "  8: 'ru',\n",
       "  9: 'sw',\n",
       "  10: 'th',\n",
       "  11: 'tr',\n",
       "  12: 'ur',\n",
       "  13: 'vi',\n",
       "  14: 'zh'},\n",
       " 'lang2id': {'ar': 0,\n",
       "  'bg': 1,\n",
       "  'de': 2,\n",
       "  'el': 3,\n",
       "  'en': 4,\n",
       "  'es': 5,\n",
       "  'fr': 6,\n",
       "  'hi': 7,\n",
       "  'ru': 8,\n",
       "  'sw': 9,\n",
       "  'th': 10,\n",
       "  'tr': 11,\n",
       "  'ur': 12,\n",
       "  'vi': 13,\n",
       "  'zh': 14},\n",
       " 'n_langs': 15,\n",
       " 'n_words': 95000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model / reload weights\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "model.load_state_dict(reloaded['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter layer_norm15.0.weight not found.\n",
      "Parameter layer_norm15.0.bias not found.\n",
      "Parameter encoder_attn.0.q_lin.weight not found.\n",
      "Parameter encoder_attn.0.q_lin.bias not found.\n",
      "Parameter encoder_attn.0.k_lin.weight not found.\n",
      "Parameter encoder_attn.0.k_lin.bias not found.\n",
      "Parameter encoder_attn.0.v_lin.weight not found.\n",
      "Parameter encoder_attn.0.v_lin.bias not found.\n",
      "Parameter encoder_attn.0.out_lin.weight not found.\n",
      "Parameter encoder_attn.0.out_lin.bias not found.\n",
      "Parameter layer_norm15.1.weight not found.\n",
      "Parameter layer_norm15.1.bias not found.\n",
      "Parameter encoder_attn.1.q_lin.weight not found.\n",
      "Parameter encoder_attn.1.q_lin.bias not found.\n",
      "Parameter encoder_attn.1.k_lin.weight not found.\n",
      "Parameter encoder_attn.1.k_lin.bias not found.\n",
      "Parameter encoder_attn.1.v_lin.weight not found.\n",
      "Parameter encoder_attn.1.v_lin.bias not found.\n",
      "Parameter encoder_attn.1.out_lin.weight not found.\n",
      "Parameter encoder_attn.1.out_lin.bias not found.\n",
      "Parameter layer_norm15.2.weight not found.\n",
      "Parameter layer_norm15.2.bias not found.\n",
      "Parameter encoder_attn.2.q_lin.weight not found.\n",
      "Parameter encoder_attn.2.q_lin.bias not found.\n",
      "Parameter encoder_attn.2.k_lin.weight not found.\n",
      "Parameter encoder_attn.2.k_lin.bias not found.\n",
      "Parameter encoder_attn.2.v_lin.weight not found.\n",
      "Parameter encoder_attn.2.v_lin.bias not found.\n",
      "Parameter encoder_attn.2.out_lin.weight not found.\n",
      "Parameter encoder_attn.2.out_lin.bias not found.\n",
      "Parameter layer_norm15.3.weight not found.\n",
      "Parameter layer_norm15.3.bias not found.\n",
      "Parameter encoder_attn.3.q_lin.weight not found.\n",
      "Parameter encoder_attn.3.q_lin.bias not found.\n",
      "Parameter encoder_attn.3.k_lin.weight not found.\n",
      "Parameter encoder_attn.3.k_lin.bias not found.\n",
      "Parameter encoder_attn.3.v_lin.weight not found.\n",
      "Parameter encoder_attn.3.v_lin.bias not found.\n",
      "Parameter encoder_attn.3.out_lin.weight not found.\n",
      "Parameter encoder_attn.3.out_lin.bias not found.\n",
      "Parameter layer_norm15.4.weight not found.\n",
      "Parameter layer_norm15.4.bias not found.\n",
      "Parameter encoder_attn.4.q_lin.weight not found.\n",
      "Parameter encoder_attn.4.q_lin.bias not found.\n",
      "Parameter encoder_attn.4.k_lin.weight not found.\n",
      "Parameter encoder_attn.4.k_lin.bias not found.\n",
      "Parameter encoder_attn.4.v_lin.weight not found.\n",
      "Parameter encoder_attn.4.v_lin.bias not found.\n",
      "Parameter encoder_attn.4.out_lin.weight not found.\n",
      "Parameter encoder_attn.4.out_lin.bias not found.\n",
      "Parameter layer_norm15.5.weight not found.\n",
      "Parameter layer_norm15.5.bias not found.\n",
      "Parameter encoder_attn.5.q_lin.weight not found.\n",
      "Parameter encoder_attn.5.q_lin.bias not found.\n",
      "Parameter encoder_attn.5.k_lin.weight not found.\n",
      "Parameter encoder_attn.5.k_lin.bias not found.\n",
      "Parameter encoder_attn.5.v_lin.weight not found.\n",
      "Parameter encoder_attn.5.v_lin.bias not found.\n",
      "Parameter encoder_attn.5.out_lin.weight not found.\n",
      "Parameter encoder_attn.5.out_lin.bias not found.\n"
     ]
    }
   ],
   "source": [
    "from src.model import build_model\n",
    "\n",
    "params.encoder_only = False\n",
    "params.reload_emb = ''\n",
    "params.reload_model = '{0},{0}'.format(model_path)\n",
    "params.local_rank = -1\n",
    "\n",
    "en, de = build_model(params, dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE.\n",
    "\n",
    "Below you can see an example for English, French, Spanish, German, Arabic and Chinese sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = \"codes_enfr\" # path to the codes of the model\n",
    "# codes = \"\"\n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    # write sentences to tmp file\n",
    "    with open('/tmp/sentences', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    \n",
    "    # apply bpe to tmp file\n",
    "    os.system('%s applybpe /tmp/sentences.bpe /tmp/sentences %s' % (fastbpe, codes))\n",
    "    \n",
    "    # load bpe-ized sentences\n",
    "    sentences_bpe = []\n",
    "    with open('/tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())\n",
    "    \n",
    "    return sentences_bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dancing once he had worn trendy itali@@ an leather shoes and jeans from paris that had cost three hundred euros .']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['dancing once he had worn trendy italian leather shoes and jeans from paris that had cost three hundred euros .']\n",
    "print(to_bpe(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once he had worn trendy itali@@ an leather shoes and jeans from paris that had cost three hundred euros .\n",
      "\n",
      "羅@@ 伯@@ 特 · 皮@@ 爾 斯 生@@ 於 18@@ 63@@ 年 , 在 英@@ 國 曼@@ 徹@@ 斯@@ 特 學@@ 習 而 成@@ 為 一 位 工@@ 程@@ 師 . 19@@ 33@@ 年 , 皮@@ 爾@@ 斯 在 直@@ 布@@ 羅@@ 陀@@ 去@@ 世 .\n",
      "Number of out-of-vocab words: 22/66\n"
     ]
    }
   ],
   "source": [
    "# Below are already BPE-ized sentences\n",
    "\n",
    "# list of (sentences, lang)\n",
    "sentences = [\n",
    "    'once he had worn trendy italian leather shoes and jeans from paris that had cost three hundred euros .', # en\n",
    "    #'Le français est la seule langue étrangère proposée dans le système éducatif .', # fr\n",
    "    #'El cadmio produce efectos tóxicos en los organismos vivos , aun en concentraciones muy pequeñas .', # es\n",
    "    #'Nach dem Zweiten Weltkrieg verbreitete sich Bonsai als Hobby in der ganzen Welt .', # de\n",
    "    #'وقد فاز في الانتخابات في الجولة الثانية من التصويت من قبل سيدي ولد الشيخ عبد الله ، مع أحمد ولد داداه في المرتبة الثانية .', # ar\n",
    "    '羅伯特 · 皮爾 斯 生於 1863年 , 在 英國 曼徹斯特 學習 而 成為 一 位 工程師 . 1933年 , 皮爾斯 在 直布羅陀去世 .', # zh\n",
    "]\n",
    "\n",
    "# bpe-ize sentences\n",
    "sentences = to_bpe(sentences)\n",
    "print('\\n\\n'.join(sentences))\n",
    "\n",
    "# check how many tokens are OOV\n",
    "n_w = len([w for w in ' '.join(sentences).split()])\n",
    "n_oov = len([w for w in ' '.join(sentences).split() if w not in dico.word2id])\n",
    "print('Number of out-of-vocab words: %s/%s' % (n_oov, n_w))\n",
    "\n",
    "# add </s> sentence delimiters\n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split()) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italian']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['once he had worn trendy italian leather shoes and jeans from paris that had cost three hundred euros .']\n",
    "sentences = to_bpe(sentences)\n",
    "[w for w in ' '.join(sentences).split() if w not in dico.word2id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "slen = max([len(sent) for sent in sentences])\n",
    "\n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "for i in range(len(sentences)):\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i]])\n",
    "    word_ids[:len(sent), i] = sent\n",
    "\n",
    "lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "                             \n",
    "# NOTE: No more language id (removed it in a later version)\n",
    "# langs = torch.LongTensor([params.lang2id[lang] for _, lang in sentences]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None\n",
    "langs = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2, 1024])\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tensor` is of shape `(sequence_length, batch_size, model_dimension)`.\n",
    "\n",
    "`tensor[0]` is a tensor of shape `(batch_size, model_dimension)` that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is this vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
